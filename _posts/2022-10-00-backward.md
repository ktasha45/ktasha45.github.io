---
layout: post
title:  backpropagation은 어떻게 이뤄질까?
categories: dl
---

## 목표
mlp의 역전파를 이루는 4개 수식을 이해해보자. (Four fundemental equations behind backpropagation)
$$\delta^L = \nabla_aC \ \odot \ \sigma'(z^L)$$
$$\delta^l = \left(\left(w^{l+1}\right)^T\delta^{l+1}\right) \ \odot \ \sigma'(z^l)$$
$$\frac{\partial C}{\partial b_j^l} = \delta^l_j$$
$$\frac{\partial C}{\partial w_{jk}^l} = a^{l-1}_k\delta^l_j$$

## backpropagation
신경망의 학습이 이뤄지기 위해선 $\partial C/\partial w$를 구해야 한다. ($C$는 cost function이다)
어떻게 구할 수 있을까?  
무식하게 모든 $\mathbf w$에 대해 $$\lim_{h \rightarrow 0}\frac{L_{x}(w+h) - L_x(w-h)}{2h}$$
를 구할 수는 없다.  
역전파를 사용하면 수치미분을 쓰지 않고, 말 그대로 오차($\delta$)를 앞으로 보내면서 각 가중치들의 gradiet 를 구할 수 있다. 오차를 앞으로 보낸다고 하여 오차역전파이다. (뇌피셜임)  
어떻게 그게 가능한지 알아보자.

## notation, review
이 글은 [neuralnetworksanddeeplearning](http://neuralnetworksanddeeplearning.com/chap2.html) 이라는 사이트를 참고해서 쓰인 글이다. 표기 또한 원문을 그대로 따라갈 것이다.  
이 챕터에선 feedforward 연산을 수식으로 정리하면서 앞으로 사용할 notation에 대해 소개한다.  

다음 수식을 먼저 보고, 아래 notation에 대한 설명을 보는 것이 이해하기 쉽다.
$$a_j^l = \sigma\left(b_j^l + \sum_k{w_{jk}^l}a_k^{l-1}\right)$$

 - $w_{jk}^l$ : $(l-1)$<sup>th</sup> 층의 $k$<sup>th</sup> 번째 뉴런에서 $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런으로 가는 가중치. mlp에서 노드들은 바로 이전 노드와 연결된다는 것이 자명하므로 $(l-1)$<sup>th</sup> 층에서 온다는 것을 굳이 표기하진 않음.
 - $b_j^l$ : $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 bias
 - $a_j^l$ : $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 활성화 값(출력)

첫 번째 식을 보면, $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 출력 $a_j^l$는 $(l-1)$<sup>th</sup> 층의 activation(출력) 들과 연결된다.

vector와 matrix를 사용해서 표기를 간결하게 해보자.
$$a^l = \sigma\left(w^la^{l-1}+b^l\right)$$

 - $w^l$ : $l$<sup>th</sup> 층과 연결되는 모든 weight들 (matrix)  
 $w^l$ 의 $j$ 행의 $k$ 열의 값이 $w_{jk}^l$ 임.
 - $b^l$ :  $l$<sup>th</sup> 층의 모든 bias들 (vector)
 - $a^l$ : $l$<sup>th</sup> 층의 모든 활성화 값(출력)
 - We use the obvious notation $\sigma(v)$ to denote this kind of **elementwise** application of a function. 비선형함수는 elementwise로 적용된다고 생각한다.
   - $\sigma(v)_j = \sigma(v_j)$

마지막으로
$$z_j^l = b_j^l + \sum_k{w_{jk}^l}a_k^{l-1}$$
라고 하자.

## deep dive
구해야 하는 것은 $\frac{\partial C}{\partial w_{jk}^l}$ 과 $\frac{\partial C}{\partial b_j^l}$ 이다. 이 2개를 구하기 위해서 intermediate quantity $\delta^l_j$ 를 도입하려 한다.  
이는 이는 $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 **error** 를 의미한다. 역전파는 이 error $\delta^l_j$를 구하고, 그리고 $\delta^l_j$를 통해서 $\frac{\partial C}{\partial w_{jk}^l}$ 과 $\frac{\partial C}{\partial b_j^l}$ 를 구하는 방식으로 작동한다.

