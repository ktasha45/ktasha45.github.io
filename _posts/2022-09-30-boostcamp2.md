---
layout: post
title:  boostcamp ai tech 4기 2주차 회고
categories: boostcamp
---

# boostcourse ai tech 4기 2주차 회고
## learned
- pytorch, python 문법, pytorch templete

## boostcamp
- 두런두런(변성윤 마스터 - 함께 자라기. 수), 오피스 아워(과제 해설. 목), 스페셜 피어세션(금), 마스터 클래스(최성철 마스터. 금)
- 마클 때 DE 로서는, 다양한 backend 관련 지식과 경험이 중요하다 언급되었음. 이론 공부를 하기도 바쁜데, 그거까지 어떻게 할 수 있을지 모르겠음.
- pytorch의 gather, scatter, hook, apply 등은 처음 사용해봤음.
- 수요일 1시 멘토링 했음 - 주말에 질문들 다시 보고 생각해보자. 특히 KL divergence, Monte-Carlo 쪽을 보자. 
- 사실 그 전에 1주차 강의 먼저 다 들어야 함. 필기하면서 듣다보니 오래 걸리는 것 같음. 주말에 들을 시간이 없는데
- sequence 가 길어지면 lstm이 transformer보다 연산량이 많다는데, 잘 이해가 안 됨. lstm의 파라미터가 sequence가 늘어날 수록 많아진다는데... 파라미터 shape은 고정되어야 할 텐데 무슨 의미인지
- 금요일 10시~10시 40분 cv-nlp-rl 논문 리뷰 참관
  - batchnormalization
  - Augraphy : A Data Augmentation Library for Document Images
- 7강부터 9강 (tensorboard, wandb, multi gpu, ray tune) 은 프로젝트를 실제로 진행할 때 중요할 것 같음. genielabs 할 때도 HPO 자동화가 힘들었음
  - metric을 제공하지 않는 대회였고, 그래서 custom metric이 바뀔 때마다 다시 일일이 실험을 해줘야 했음
- 스페셜 피어세션 때는 미국 대학을 나온 사람도 있었고, 물리학과를 졸업하고 CV 관련 회사에서 일하다 온 사람도 있었음.
- 이번 주는 이론적인 건 딱히 없었음. pytorch, python 관련 문법을 공부했음.
- 월요일 d2l.ai 17장, templete 스터디
- **lstm 이 transformer 보다 모델 사이즈가 클 수가 있다** 는 것이 잘 이해가 되지 않음.
  - 멘토님 설명을 듣고 나서 찾아봤는데, time-step 관련 내용은 많지 않았음
  - DL basic에서 나오는 걸까? 지금 당장은, rnn은 $h_t$를 update할 때에 $h_{t-1}$만 고려할 수도 있지만 time-step 을 잘 조정하면 $h_{t-1}, h_{t-2}, ...$ 처럼 더 많은 hidden state 정보를 사용할 수 있다고만 이해했음.
  - 따라서, 이 time-step 을 키우다보면 transformer 보다 연산량이 많아질 수 있다는 식으로 이해했는데, 관련된 내용을 찾기 어려움
  - 내가 알기로는 $h_{t-1}$이 $h_{t-2}, h_{t-3}, ...$의 모든 정보를 이미 함축하고 있다고 이해하고 있음
  - 일단 3주차 rnn 쪽을 듣고 생각해보자. 지금은 대체 누구 말이 맞는 건지.. 모르겠음 자료가 없으니

## not boostcamp
- backend 는 학교 수업에서 배우는 게 좋을 것 같음
- 지금 부스트캠프를 하면서는 자동화 부분 (언급한 7-9강)만 공부하고 - 이건 실제로 프로젝트에서 쓰이니 - 스파크, DB, 리눅스 등은 그 이후에 하자
- oversampling 에서 같은 데이터를 계속 쓰는 것은 overfitting을 불러일으킬 수 있음 - augmentation 을 적용
- smote 와 augmentation 의 차이는? - smote 는 knn 등의 기법을 사용함
  - nlp는 task마다 augmentation 기법이 다를 것임. smote는 그닥 효과가 좋지 못하다고 함. 
  - 대표적인 nlp 의 augmentation - back translation, paraphrase, similar word/span replacement
  - smote 자체가 전제하고 있는 통계적 분포가 있다고 함 - ?
- batchnorm을 좀 더 확실하게 공부하고 싶음 - batchnorm 이후 relu를 적용할 때, batchnorm은 relu 그래프에서 0 앞쪽, 뒤쪽에 얼마나 데이터를 분포시킬 것인지를 학습한다? - 혁펜하임
  - -> 그럼 feature selection이라 생각할 수도 있을 것 같음 - l1 norm?
- sigmoid, softmax, odds, logit, OLS 등.. 통계
  - sigmoid를 사용한다는 것은, 결국 모델의 출력을 logit으로 보겠다는 의미이다.
- python decorator.. python 문법도 언제 한번 책을 보든지 해서 공부해보고 싶음. python clean code나 design pattern도
- to be, as is 라는 걸 처음 봤음

## miscellaneous
- genielabs 발표만 남음
- kisti 일요일 회의
- 이번 주 동아리 MT - 재미있었음
- 수학 공부 - prml
- 푸리에, gnn 등등.... 할 시간이 없음. 미뤄야 함. 당장 다음주에 AAE, VIT 등을 배우니- 그것만 확실히 공부하는 것도 힘들 듯. prml이나 꾸준히 계속 읽자. 이건 어차피 언젠간 읽어야 할 책이니
- genielabs가 끝나면 kaggle dacon 을 꾸준히 계속 나가자
- algorithm study 존속 여부 - 
- 

## feeling
- 대회 마감 때문에 루틴이 깨졌는데 다시 찾으면 좋겠다
- 방향을 못 잡겠다
- 조급해하지 말자