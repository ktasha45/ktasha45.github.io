---
layout: post
title:  mml 1
categories: mathematics
---

- 2022/03/15 스터디 1주차
## Polynomial curve fitting
- $\mathbf x \in \mathbb R^n, \mathbf t \in \mathbb R^n$
- $y(x, \mathbf w) = \sum_{j=0}^M w_jx^j$
  - hyperparameter $M$ 을 어떻게 정해야 하는가? - overfitting, generalization
- $E(\mathbf w) = \frac{1}{2} \sum_{n=1}^N \\{y(x_n, \mathbf w) - t_n\\}^2$
- $E_{\text{RMS}} = \sqrt{2E(\mathbf w^*) / N}$
  - independent from size of data set
  - same scale with target value $\mathbf t$
- $M$ 이 커질수록 polynomial curve 가 oscillating 하게 되고, 이는 overfitting 을 야기 - 차수가 커질수록 유연하고, random noise 에 tuned
- $\tilde E(\mathbf w) = \frac{1}{2} \sum_{n=1}^N (y(x_n, \mathbf w) - t_n)^2 + \frac{\lambda}{2} \vert\vert\mathbf w\vert\vert^2$
  - shrinkage method in statistics literature
  - regression
  - ridge regression - quadratic regularizer
  - weight decay in the context of neural network

## Probability Theory
- $x = g(y), \ p_y(y) = p_x(x) \rvert \frac{dx}{dy}\lvert = p_x(g(x))g'(x)$
  - $p_x(x)\delta x \approxeq p_y(y)\delta y$
  - Nonlinear change of variable, Jacobian factor
  - The concept of the maximum of a probability density is dependent on the choice of variable
- $\mathbb E[f] = \sum_x p(x)f(x)$
- $\mathbb E[f\vert y] = \sum_x p(x\vert y)f(x, y)$
- $\mathbb E_x[x, y]$ is a function of $y$
- $\text{cov}[\mathbf x, \mathbf y] = \mathbb E_{\mathbf x, \mathbf y}[\mathbf x\mathbf y^\intercal] - \mathbb E[\mathbf x] \mathbb E[\mathbf y]$
- $p(\mathbf w \vert \mathcal D) = \frac{p(\mathbf D\vert\mathbf w)p(\mathbf w)}{p(\mathbf D)}$
  - $p(\mathcal D) = \int p(\mathcal D\vert\mathbf w)p(\mathbf w) d\mathbf w$
  - posterior $\propto$ likelihood $\times$ prior
- frequentist vs bayesian
  - likelihood
  - estimator, estimate, error bars
    - maximum likelihood
    - bootstrap
- MLE 는 구조적으로 variance 를 과소평가함 - precision 을 과대평가함 -> overfitting

## Model Selection
- validation
  - To prevent the overfitting to test data in HPO
- cross-validation, LOOCV
- Akaike information criterion, Bayesian information creierion
  - MLE 의 bias 를 막기 위해 많은 information criteria 가 등장
  - AIC
    - $\text{ln} (p(\mathcal D \vert \mathbf w_{\text{ML}})) - M$
    - 통계 시간에는 $-2\text{ln(L)}+2k$ 로 배우고 작을수록 좋다고 했는데-
    - $\text{ln}(L)$ 은 모델의 적합도(가능도)이고, $k$ 는 파라미터 수이다
    - 말 그대로 파라미터 개수를 패널티로 추가한 것이다
  - BIC, SIC (Schwarz information criterion)
    - $k\text{ln}(n) - 2\text{ln} (p(\mathcal D \vert \mathbf w_{\text{ML}})$
    - AIC 에서 $M$ 을 $k\text{ln}(n)$ 으로 바꾼 형태
    - 패털티가 더 큼
- model selection 은 정보 이론과 깊은 연관을 맺고 있음.

---

first draft: 2022.03.15 23:44