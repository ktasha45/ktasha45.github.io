---
layout: post
title:  prml 1
categories: mathematics
---

- 2022/03/15 스터디 1주차
## Polynomial curve fitting
- $\mathbf x \in \mathbb R^n, \mathbf t \in \mathbb R^n$
- $y(x, \mathbf w) = \sum_{j=0}^M w_jx^j$
  - hyperparameter $M$ 을 어떻게 정해야 하는가? - overfitting, generalization
- $E(\mathbf w) = \frac{1}{2} \sum_{n=1}^N \\{y(x_n, \mathbf w) - t_n\\}^2$
- $E_{\text{RMS}} = \sqrt{2E(\mathbf w^*) / N}$
  - independent from size of data set
  - same scale with target value $\mathbf t$
- $M$ 이 커질수록 polynomial curve 가 oscillating 하게 되고, 이는 overfitting 을 야기 - 차수가 커질수록 유연하고, random noise 에 tuned
- $\tilde E(\mathbf w) = \frac{1}{2} \sum_{n=1}^N (y(x_n, \mathbf w) - t_n)^2 + \frac{\lambda}{2} \vert\vert\mathbf w\vert\vert^2$
  - shrinkage method in statistics literature
  - regression
  - ridge regression - quadratic regularizer
  - weight decay in the context of neural network

## Probability Theory
- $x = g(y), \ P_y(y) = P_x(x) \rvert \frac{dx}{dy}\lvert = P_x(g(x))g'(x)$
  - $p_x(x)\delta x \approxeq p_y(y)\delta y$
  - Nonlinear change of variable, Jacobian factor
  - The concept of the maximum of a probability density is dependent on the choice of variable
  - But why use absolute value of the amount of change?
- $\mathbb E[f] = \sum_x P(x)f(x)$
- $\mathbb E[f\vert y] = \sum_x P(x\vert y)f(x, y)$
- $\mathbb E_x[x, y]$ is a function of $y$
- $\text{cov}[\mathbf x, \mathbf y] = \mathbb E_{\mathbf x, \mathbf y}[\mathbf x\mathbf y^\intercal] - \mathbb E[\mathbf x] \mathbb E[\mathbf y]$
- $P(\mathbf w \vert \mathcal D) = \frac{P(\mathbf D\vert\mathbf w)P(\mathbf w)}{P(\mathbf D)}$
  - $P(\mathcal D) = \int P(\mathcal D\vert\mathbf w)p(\mathbf w) d\mathbf w$
  - posterior $\propto$ likelihood $\times$ prior
- frequentist vs bayesian
  - likelihood
  - estimator, estimate, error bars
    - maximum likelihood
    - bootstrap
- MLE 는 구조적으로 variance 를 과소평가함 - precision 을 과대평가함 -> overfitting

## Model Selection
- validation
  - To prevent the overfitting to test data in HPO
- cross-validation, LOOCV
- Akaike information criterion, Bayesian information creierion
  - MLE 의 bias 를 막기 위해 많은 information criteria 가 등장
  - AIC
    - $\text{ln} (P(\mathcal D \vert \mathbf w_{\text{ML}})) - M$
    - 통계 시간에는 $-2\text{ln(L)}+2k$ 로 배우고 작을수록 좋다고 했는데-
    - $\text{ln}(L)$ 은 모델의 적합도(가능도)이고, $k$ 는 파라미터 수이다
    - 말 그대로 파라미터 개수를 패널티로 추가한 것이다
  - BIC, SIC (Schwarz information criterion)
    - $k\text{ln}(n) - 2\text{ln} (P(\mathcal D \vert \mathbf w_{\text{ML}})$
    - AIC 에서 $M$ 을 $k\text{ln}(n)$ 으로 바꾼 형태
    - 패널티가 더 큼
- model selection 은 정보 이론과 깊은 연관을 맺고 있음.

## The Curse of Dimensionality
- feature 개수가 $D$ 개이고, $M$ 차원 polynomial curve fitting 을 한다고 하면, 파라미터 개수는 $D^M$ 개가 필요하다.
- exponential 이 아니긴 하지만 power law growth 도 간과하긴 어렵다
- 원점을 중심으로 두는 구체(sphere) 를 생각했을 때, 구체가 존재하는 차원의 차수가 커질수록 껍질에 대부분의 volume 이 집중되는 현상을 관찰할 수 있다.
  - 책의 figure 1.22
  - figrue 1.23 는, $D$ 차원 정규분포를 극좌표계로 바꾼 뒤 with respect to $\theta$ 로 summation 한 분포를 그린 것이다.
    - 신기하게도 $D$ 가 커질수록 바깥쪽으로 분포가 쏠리는 현상을 관찰할 수 있다

## Decision Theory
- $p(\text{mistake}) = \int_{\mathcal R_1}p(\mathbf x, \mathcal C_2)d\mathbf x + \int_{\mathcal R_2}p(\mathbf x, \mathcal C_1)d\mathbf x$
- $p(\text{correct}) = \sum_{k=1}^K\int_{\mathcal R_k}p(\mathbf x, \mathcal C_k)d\mathbf x$
- $\mathbb E[L] = \sum_k \sum_j \int_{\mathcal R_j} L_{kj}p(\mathbf x, \mathcal C_k)d\mathbf x$
  - $L_{kj}$ 는 loss matrix - 혼동 행렬과 비슷한 형태, 각 element 가 가중치를 의미함
  - 이걸 최소화하는 decision boundary 를 구해야 하고, 이는 bayesian Decision Theory 등을 사용해 구할 수 있다. 책에서는 다루지 않았음.
    - $\sum_k L_{kj}P(\mathcal C_k \vert \mathbf x)$ 을 각 $\mathcal R_j$ 마다 구하고 가장 작은 것을 선택. $P(x)$ common factor 이므로 제거.
      - $$g_i(\mathbf x) = P(c_i \vert \mathbf x) \\ \rightarrow g_i(\mathbf x) = P(\mathbf x \vert c_i)P(c_i) \\ \rightarrow g_i(\mathbf x) = \text{ln}P(\mathbf x \vert c_i) + \text{ln} P(c_i)$$
      - 이런 느낌? $P(\mathbf x \vert c_i)$ 을 normal distribution 등으로 가정하고 풀 수 있음 - closed form
        - 분산이 arbitrary 여도 구할 수 있음
  - 저 expected loss 는 아주 general 한 개념임. 딥러닝에서 loss function 에 가중치를 부여하는 것과 비슷하다고 볼 수도 있을 듯? 가령 focal loss 처럼
    - 음.. 더 생각해봐야겠다
  - 위에서 이야기한 likelihood 를 normal distribution 으로 보는 것은 아주 특별한 경우이다.
    - 분포가 연속이라면 결정 경계를 파라미터로 두고 gradient descent 등으로 최적화할 수 있을 것이다
    - 하지만 분포가 불연속이라면, 최적화할 수 없다 - 불연속일 때의 최적화 방법이 따로 있을까?
      - 따라서 결졍 경계를 하이퍼파라미터로 두고 grid search 등을 통해 구해야 할 것임. 근데 뭐 이렇게 하는 경우가 있을지 모르겠다. 어디까지나 원론적인 이야기임
  - class conditional density 를 구하고 결정 경계를 구하고, 2단계로 나눠서 classification 수행
- reject option 이라는 것이 있어서, $P(\mathcal R_1 \vert \mathbf x)$ 와 $P(\mathcal R_2 \vert \mathbf x)$ 분포 사이의 애매한 부분에서는 결정을 포기할 수 있음
  - 어떤 지점에서의 확률이 특정 threshold 에 도달하지 못하면 reject
  - $k$ 개의 클래스가 있을 때 threshold 는 $\frac{1}{k}$ 보다 크거나 같고 1보다 작을 것임
  - 기대 손실을 줄이는 데에 활용 가능
- methods
  - conditional distribution of $x$ given $C_k$ $P(\mathbf x\vert C_k)$ 를 통해 posterior 를 모델링
    - generative model
    - 데이터의 분포를 추정하므로 outlier detection 등에 사용 가능
  - posterior 를 직접적으로 모델링 - class conditional density 와 prior 르 간접적으로 모델링한 위의 경우와는 다름
    - discriminative mmodel
  - posterior 추정 X. 그냥 바로 판단하는 decision maker 를 설계
    - discriminant function
- regression 에서의 expected loss
  - $\mathbb E[L] = \int \int L(t, y(\mathbf x)) p(\mathbf x, \mathcal C_k)d\mathbf x$
  - $L(t, y(\mathbf x))$ 를 MSE loss 로 두고 변분법을 통해 optimal $y(\mathbf x)$ 를 구하면,
    - $y(\mathbf x) = \mathbb E[t\vert \mathbf x]$
  - 이 식을 원래 $\mathbb E[L]$ 에 대입하고 정리하면
    - $\mathbb E[L] = \int \{y(\mathbf x) - \mathbb E[t\vert \mathbf x]\}^2p(\mathbf x)d \mathbf x + \int \{\mathbb E[t\vert \mathbf x] - t\}^2p(\mathbf x)d \mathbf x $
    - 첫 번째 항은 예측값과 target 의 loss 이고 우리는 이걸 최소화시키는 방향으로 최적화하므로 사라질 것임
      - 남는 것은 두 번째 항이고, 이는 target 자체의 noise 를 의미함
      - 줄일 수 없는, loss 의 lower bound
      - 이건 least square 의 기하학적 의미를 생각해보면, 애초에 noise 를 제거하기 위해 mse 로 fitting 시키는 것임을 대충 이해할 수 있다
  - MSE 말고도 다양한 loss 사용 가능
    - MAE 랑 MSE 실험하고 비교했던 거 있는데 어디 있는지 모르겠음

## Information Theory
- 정보: 놀라움의 정도, 불확실성의 정도
  - 놀라울수록 정보량이 커진다, 불확실할수록 정보량이 커진다는 게 직관적으로 쉽게 연결되지 않을 수도 있음 (불확실할수록 정보량이 크다- 라는 것이)
    - 그냥 불확실성의 정도, 놀라움의 정도라고 생각하는 것이 가장 마음 편함
- $h$ 가 어떤 사건의 정보를 나타내는 함수일 때, 두 사건 x, y 가 독립이라면
  - $h(x, y) = h(x) + h(y)$ 이면 좋을 것임. 선형적인 놀라움의 증가 - 직관과 맞아떨어짐
  - 그리고, 가정에 따라 $p(x, y) = p(x)p(y)$
  - 여기에 더해 확률이 작을수록, 놀라움의 정도는 커져야 함
  - $\rightarrow$ negative log!
- $h(x) = -\log_2 p(x)$ 이고
  -  이는 데이터를, 정보를 송수신할 때 필요한 비트의 양을 의미하기도 함 - 정보량
  - 이번엔 RV $X$ 가 있을 때, 이걸 송신할 때 필요한 정보량의 기댓값을 생각해보면
    - $H[X] = -\sum_{x \in X} p(x)\log_2p(x)$
  - 이게 엔트로피임
    - 이산확률분포일 때 엔트로피는 uniform 일 때 가장 큼. 분포에 피크가 있을 때 엔트로피가 낮아짐
      - skewed 일 때는? 별 상관없나
    - 연속확률분포일 때는 가우시안이 가장 엔트로피가 큼
      - mean, std 에 제약을 주고 변분추론!
- 실제 분포 $p$ 를 근사해서 $q$ 를 만들었다고 해보면
  - 이때 $q$ 는 실제 분포를 설명함에 있어서 실제 분포보다 항상 inferior 함
  - 이때 $p$ 를 $q$ 로 나타냈을 때, 추가되는 불확실성의 정도를 정량화해보면
    - $-\int p(\mathbf x)\ln q(\mathbf x)d\mathbf x - (-\int p(\mathbf x)\ln p(\mathbf x)d\mathbf x)$
    - 이렇게 볼 수 있음. 첫 번째 term 이 $q$ 로 $p$ 를 표현하고자 했을 때 나오는 불확실성이고, 두 번째 항은, 그냥 실제 분포의 불확실성임
  - 조금 더 전개해보면, $\int p(\mathbf x) \ln\{ \frac{q(\mathbf x)}{p(\mathbf x)}\}d\mathbf x$ 이고 이를 KL-divergence 라고도 부름
    - 항상 0보다 크고 asymmetric 함
      - 따라서 거리의 개념은 아님. 하지만 두 분포 사이의 거리, 라고 생각하는 것은 이해에 도움을 줌
- 두 RV $X, Y$ 가 있을 때 둘이 얼마나 종속되어있는가를 살펴보기 위해 KL-divergence 를 사용할 수 있는데
  - $P(X, Y)$ 를 $P(X)P(Y)$ 로 나타낼 때 추가되는 불확실성의 정도가, 두 RV 간의 종속성의 정도를 나타낸다는 것을 직관적으로 받아들일 수 있음
  - 이를 식으로 표현해보면
    - $I[X, Y] = \text{KL}(P(X, Y) \vert\vert P(X)P(Y))$
    - 식을 잘 전개해보면 $I[X, Y] = H[X] - H[X\vert Y] = H[Y] - H[Y\vert X]$
  - 이는 DT 에서 가지를 분기하는 기준이 되는 infromation gain 을 의미하기도 하고, 보통은 mutual information 이라고 표현함. InfoNCE 같은 SSL 기법에서 이런 상호 정보량에 기반한 방법론을 사용함
  - 두 번째 식은, 베이지안의 관점에서 - $Y$ 를 evidence 로 보고, $Y$ 가 $X$ 를 얼마나 잘 설명하는가 - 라고 생각할 수도 있음
- KLdiv loss 와 NLLloss 는 같다! 어차피 $p$ 는 항상 1임

---

first draft: 2022.03.15 23:44  
second draft: 2022.03.22 08:44  
( + Decision Theory, Information Theory)