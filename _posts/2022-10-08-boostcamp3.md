---
layout: post
title:  boostcamp ai tech 4기 3주차 회고
categories: boostcamp
---

# boostcamp ai tech 4기 3주차 회고
22.10.03 - 22.10.09
## learned
- DL basic (mlp, cnn, rnn, transformer, vit, aae)

## boostcamp
- batchnorm 이후 relu를 적용할 때, batchnorm은 relu 그래프에서 0 앞쪽, 뒤쪽에 얼마나 데이터를 분포시킬 것인지를 학습한다?
  - 저번 주에 나왔던 질문.  
  batchnorm을 relu(activation) 이후에 쓰기도 하고, 그게 성능이 더 좋은 경우도 있다고 함.
  - 다른 캠퍼님이 DM을 보내셔서 batchnorm 관련 질답을 했음. (화요일)
    - batch마다 다른 분포를 같게 해주는
    - batch size가 커지면 batch마다 분포가 크게 안달라질텐데  
      - 배치마다 다른 분포를 같게 해주는 게 아니라 -> layer의 출력을 정규화하는 데에 효과가 있다?  
      - 출력을 평균 0, std 1으로 맞춰 모아주는  
      - $\gamma$, $\beta$ 로 보정 
      - 어차피 배치마다 비슷한 출력이 나올 텐데?  
      -> 배치 사이즈가 작으면 아닐 수도 있음.
      - 내부 공변량 변화를 정말 줄이는 것인가?  
      -> 그게 아니고, loss landscape를 평탄하게 하는 것이다
      - loss landscape 가 평탄해지는 것과 공변량 변화를 줄이는 것과는 연관이 없는가?
- 금요일 10시~10시 40분 cv-nlp-rl 논문 리뷰 참관
  - vision transformer
    - resnet 보다 학습 비용이 적다는데, 파라미터 수가 더 적은 것인가?
  - VQA
- 이제 플젝 팀원을 구하기 시작했다.
  - 다음 주에 오프라인 행사가 있다.
  - 10/10까지 팀원 모집 글을 써야 한다.
- level1 팀원들과 멘토링 질문에 답을 각자 달고 공유해보기로 했음.
- 일주일 동안 뭐에 꽂혀서 시간을 엄청 썼음. (lstm 관련) 아직도 잘 모르겠음. 질문을 계속 하긴 하는데 이쯤하고 그만해야 할 것 같은 분위기임.
- DL basic 뒷부분은 아직 못 들었다. 가능하면 연휴(10/10) 때 다 듣고 복습하고, 블로그에 정리할 생각이다.
- 기본과제 5(multihead attention), 심화과제 해보면 좋을 듯
- data viz 듣기

## not boostcamp
- 주말에 prml 읽어야 함.
- d2l도 준비해야 함. ppt
- crossentropyloss
  - 출력 logit은 20개, 10개로 분류하는 문제 -> 학습 잘 됨 (label index가 target)
  - 어차피 crossentropyloss는 정답의 logit만 키우는 방식이다보니 그런 것이라고 생각했는데, crossenrtopy with softmax 까지 공부를 해봐야 확실히 알 수 있을 것 같음.
  - 10번째 index 이후는 연결된 가중치, 그리고 결과값이 전부 작은 값이었음. 그렇게 학습되는 듯.
- adam, sgd 수렴 속도 차이
  - adam 은 momentum 을 사용함  
  -> 이전 그라디언트가 지금도 반영됨. rnn 에서 h 처럼.. 이전 gradient가 조금씩은 반영되는 형태라고 생각하면, 전체 데이터셋을 골고루 본다고 생각할 수도 있음.  
  -> 나는 아래로 계속 내려가는 형태(선형회귀) 라면 가속도가 붙기 때문에 더 수렴 속도가 빠를 것이라고만 생각했음.
- normalization을 왜 해야 하는가?
  - feature마다 scale을 맞춰주기 위해서
  - 만약 feature 마다 이미 scale이 같다면? (0~255 처럼)
    - 그래도 0-1으로 맞춰주는 게 뭔가 더 좋을 것 같다. loss landscape 가 평탄해질 것 같다. 왜인지는 모름.
    - 255를 그대로 쓰면 exploding gradient 문제가 생길 수도 있을 것 같다.
    - normalize 하면 전체적인 gradient 가 작아질 것 같은데 따라서 loss landscape가 평탄해진다고 해석할 수도 있는 걸까?
- positional encoding 을 왜 concat 하지 않고 add 하는가?
  - 멘토님께선 메모리의 효율성 때문이라 하셨음
  - [link](https://github.com/tensorflow/tensor2tensor/issues/1591) 관련 글인데 읽어보진 않았음
- vit 에서 positional embedding vector 는 왜 학습 가능한 파라미터인가?
  - 성능은 비슷하다는 것이 논문에 나와있다고 함(p. 6, 9)
- attention 연산에서 softmax에 값이 들어갈 때, softmax 하기 전에 QK^T를 sqrt(d_K) 로 나눠주는 이유
  - Q와 K의 내적 연산에 d가 끼치는 영향(d가 클수록 내적이 커지는 경향)을 없애기 위해서 scale 하는 것 - 조교
- bias-variance tradeoff 식 전개해보기
- mae vs. mse (이 외에도 rmse, mape 등 뭐가 많더라)
  - mse는 학습이 잘 된다. 많이 틀린 곳을 집중적으로 학습하니까
  - 다만 오차가 상대적으로 작은 쪽은 잘 학습되지 않음 (오래 학습하면 결국 fitting 되겠지만)
  - mae는 모든 곳을 비슷하게 학습함. 오차를 제곱시키지 않으므로 아웃라이어에 강할 것 같음. (이는 l1, l2 규제와도 연관될 것 같다)
- 신호 및 시스템 책을 빌렸는데 공부할 시간이 생길지 모르겠다.

## miscellaneous
- genielabs 떨어짐. 근데 알려주지도 않음. kisti 만 남았다. 이거 끝나면.. kaggle dacon 이 가장 좋을 것 같다.
- 요즘 '나라면 저렇게 안 할 텐데' 하는 생각이 많이 드는 것 같다. 조교님들을 보면서도.. 내가 이상한 건가? 잘 모르겠다.
- 할 게 너무 많다.. 다 해야할 것 같다. 멘토링 때 조급해하지 말라고 하셨는데 쉽지 않다.
- edwith 스터디를 하기로 했다.
- 강의를 일단 다 듣자. 가능하면 수요일까지  
우선순위를 매기면서 효율적으로 하자
- 학교를 다니면서 이걸 하려고 했다니.. 진짜 미친 생각이다. 휴학을 하고 모든 시간을 쏟아도 모자란 것 같다. 이번 주는 내용 자체는 아는 게 많았지만 위에서 언급한 다른 이유 떄문에.. 시간을 좀 써서 그럴지도 모르겠는데, 다음 주부터는 본격적으로 정말 처음 해보는 recsys를 배운다. 지금도 힘든데 얼마나 더 힘들어질지.. 다른 데에 신경 쓸 여유가 없어질 것 같다.
- 이제 매일 draft에서 글을 수정하는 식으로 해야할 것 같다. 금요일 밤에 한 번에 일주일 치를 쓰고 있는데 기억이 잘 안 난다.
- 그리고 이젠 월-일 의 기록을 기록할 생각이다. 글 처음 부분에 날짜를 써놓아야겠다.

## feeling
- 일주일이 참 빠르게 지나가는 것 같다.
- 잠을 잘 자는 게 정말 중요한 것 같다.  

to be added
