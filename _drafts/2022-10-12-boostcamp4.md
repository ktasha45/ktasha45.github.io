---
layout: post
title:  boostcamp ai tech 4기 4주차 회고
categories: boostcamp
---

# boostcamp ai tech 4기 4주차 회고
22.10.10 - 22.10.16
## learned
- MF, ANN, 

## boostcamp
- 월요일까지 자기소개서를 써야 해서 썼다.
- 화요일에 github 강의가 있었다. 생활코딩님이 오셨다. 제대로 들은 것 같지는 않다. - branch 실습은 해봐야 할 것 같다.
- git graph 라는 vscode extension 을 처음 알았다.
- 수요일 17-19에 zep에서 스몰톡 이벤트가 있었다. 거기서 만난 분과 팀을 하나 싶었는데 차였다.
- 목요일에 마스터클래스(안수빈)가 있었다. 
- 금요일 10시 세미나에서 transformer 리뷰를 들었음
- 퀴즈 3개를 못 풀었다.
- 미니배치를 쓰면 그 배치 안의 데이터 간의 관계까지 학습해버리는가? - 멘토님
  - MF gd 할 때 미니배치 안 쓰는 이유?
- matrix factorization 할 때 (q+bi)(p+bu) 처럼 bias를 내적 이전에 넣어주는 방법은 없을까? 내적하고 더하는 것, 더하고 내적하는 것이 크게 차이가 날까?
- 복원추출과 비복원추출은 언제 왜 쓰일까? - variance?
- tf-idf 에서 정규화할 때 문장 길이가 아니라, max_k 로 정규화해주는 이유는?
- ANN 에서 priority queue 를 쓰는 이유?

## not boostcamp
- multi-hop + pretrained bert 방법
- multi-hop fine-tuning 방법
- batch normalization 에서 모으고 퍼트리는 것이 naive 하게 퍼트리는 것과 어떤 차이가 있는 것인가? 어떻게 loss-landscape를 smoothing하는 거지?
- transformer 관련
    - positional encoding 더해줬을 때 uniqueness 가 보장됨.
    - QKV 연산을 거쳐도 위치 정보는 보존된다.
    - 인코더를 쌓을 때 FFNN layer 의 파라미터를 공유하는 방식도 있는가?
    - attention score 로 가중합하지 않고 그냥 concat 하면 어떨까? 그렇게 하고, 신경망을 통과시켜서 벡터 크기를 맞춘다면... - 굳이 그럴 필요가 있을까 싶긴 함
    - d_model 은 고정되어 있을 때, head 가 커질수록 성능이 좋아지는 경향이 있는 듯하다. 

## miscellaneous
- 월요일 대체공휴일에는 prml을 읽었다. chap3 언제 끝내지..
- 다크모드가 안 좋다고 해서 흰 배경으로 바꿨다.
- kisti 회의가 있었다. 카톡 서버가 터져서 팀원들과 연락할 수단이 사라졌다.
- 토요일 10시에 스터디를 했다. 1시간 했다. 적당한 시간인 것 같다.
  - 확률과 가능도의 차이에 대해 알아보았다.
  - 다음 발표는 나다. ppt로 준비하는 게 좋을 것 같다.
- 최성준 교수의 Bayesian Deep Learning 시작함. 이제 대충 GP 나오는데 어질어질
- 


## feeling
- 