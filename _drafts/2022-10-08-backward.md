---
layout: post
title:  backpropagation
categories: dl
---

## goal
mlp의 역전파를 이루는 4개 수식을 이해해보자.  
(Four fundemental equations behind backpropagation)  

$$\delta^L = \nabla_aC \ \odot \ \sigma'(z^L) \tag{BP1}$$  
$$\delta^l = \left(\left(w^{l+1}\right)^T\delta^{l+1}\right) \ \odot \ \sigma'(z^l) \tag{BP2}$$  
$$\frac{\partial C}{\partial b_j^l} = \delta^l_j \tag{BP3}$$  
$$\frac{\partial C}{\partial w_{jk}^l} = a^{l-1}_k\delta^l_j \tag{BP4}$$

- > Be warned, though: you **shouldn't expect to instantaneously assimilate the equations**. Such an expectation will lead to disappointment. In fact, the backpropagation equations are so rich that understanding them well **requires considerable time and patience** as you gradually delve deeper into the equations. The good news is that such patience is **repaid many times over**. And so the discussion in this section is merely a beginning, helping you on the way to a thorough understanding of the equations.

- 본문의 내용이다. 한 번에 이해하려 하지 말라고 한다.
- 처음 보는 입장에선 notation 만 봐도 어지러울 것이다. 그래도 계속 보다보면 이해되는 순간이 온다.
- > Along the way we'll return repeatedly to the four fundamental equations, and as you deepen your understanding those equations will come to seem comfortable and, perhaps, **even beautiful and natural**
- 좀 변태 같지만.. 그렇다고 한다

## introduction
- 신경망의 학습이 이뤄지기 위해선 $\partial C/\partial w$를 구해야 한다. ($C$는 cost function이다)  
어떻게 구할 수 있을까?
- 무식하게 모든 $\mathbf w$에 대해 $$\lim_{h \rightarrow 0}\frac{L_{x}(\mathbf w+h) - L_x(\mathbf w-h)}{2h}$$
를 구할 수는 없다.  
- 오차역전파 방식을 사용하면 수치미분을 쓰지 않고, 말 그대로 오차($\delta$)를 앞으로 보내면서 각 가중치들의 gradient 를 구할 수 있다.   
- 어떻게 그게 가능한지 알아보자.

## notation, preview
- 이 글은 [neuralnetworksanddeeplearning](http://neuralnetworksanddeeplearning.com/chap2.html) 이라는 사이트를 참고해서 쓰인 글이다. 표기 또한 원문을 그대로 따라갈 것이다.  
- 이 챕터에선 feedforward 연산을 수식으로 정리하면서 앞으로 사용할 notation에 대해 소개한다.  
- 다음 수식을 먼저 보고, notation에 대한 설명을 보는 것이 이해하기 쉽다.
  - $$a_j^l = \sigma\left(b_j^l + \sum_k{w_{jk}^l}a_k^{l-1}\right) \tag5$$
    - $w_{jk}^l$ : $(l-1)$<sup>th</sup> 층의 $k$<sup>th</sup> 번째 뉴런에서 $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런으로 가는 가중치. mlp에서 노드들은 바로 이전 노드와 연결된다는 것이 자명하므로 $(l-1)$<sup>th</sup> 층에서 온다는 것을 굳이 표기하진 않음.
    - $b_j^l$ : $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 bias
    - $a_j^l$ : $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 활성화 값(출력)
  - $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 출력 $a_j^l$는 $(l-1)$<sup>th</sup> 층의 activation(출력) 들과 연결된다.
- vector와 matrix를 사용해서 표기를 간결하게 해보자.
  - $$a^l = \sigma\left(w^la^{l-1}+b^l\right) \tag6$$
    - $w^l$ : $l$<sup>th</sup> 층과 연결되는 모든 weight들 (matrix)  
    $w^l$ 의 $j$ 행의 $k$ 열의 값이 $w_{jk}^l$ 임.
    - $b^l$ :  $l$<sup>th</sup> 층의 모든 bias들 (vector)
    - $a^l$ : $l$<sup>th</sup> 층의 모든 활성화 값(출력)
    - We use the obvious notation $\sigma(v)$ to denote this kind of **elementwise** application of a function.
      - $\sigma(v)_j = \sigma(v_j)$
- $$z_j^l = b_j^l + \sum_k{w_{jk}^l}a_k^{l-1} \tag7$$
- $\odot$ 은 elementwise product 를 의미한다.

## deep dive
 - 구해야 하는 것은 $\frac{\partial C}{\partial w_{jk}^l}$ 과 $\frac{\partial C}{\partial b_j^l}$ 이다.
   - 이 2개를 구하기 위해서 intermediate quantity $\delta^l_j$ 를 도입한다.  
    - 이는 이는 $l$<sup>th</sup> 층의 $j$<sup>th</sup> 뉴런의 **error** 를 의미한다.
      - $$\delta^l_j = \frac{\partial C}{\partial z^l_j} \tag8$$
      - $z^l_j$ 는 특정 뉴런의 activation 을 거치기 전 결과값이다. - 식 (7) 참고  
      이때 $z^l_j$ 에 약간의 변화($\Delta z^l_j$)를 준다고 생각해보자. ($\Delta z^l_j$ 는 1e-8 등의 상수로 생각하자)  
      그럼 $C$ 는 $\frac{\partial C}{\partial z^l_j} \Delta z^l_j$ 만큼 변할 것이다  
      - 이때 $\frac{\partial C}{\partial z^l_j}$ 가 크다면, $C$ 의 변화량 또한 클 것이다.
        - 만약 $C$ 를 낮춰야 한다면, $\frac{\partial C}{\partial z^l_j}$ 의 반대 부호를 가지는 $\Delta z^l_j$ 를 선택하면 되고, $C$ 를 꽤 많이 줄일 수 있을 것이다.
      - 반대로 $\frac{\partial C}{\partial z^l_j}$ 가 작다면 C의 변화량 또한 작아질 것이다.
        - $\Delta z^l_j$ 만큼 $z^l_j$ 가 변해도 $C$ 는 크게 변하지 않는다는 것이다.
        - 이를 반대로 생각하면, $z^l_j$ 가 이미 optimal 에 도달했다고도 해석할 수 있다
      - $\frac{\partial C}{\partial z^l_j}$ 가 크면 $z^l_j$ 가 개선될 여지가 존재한다는 것이고, 오차가 존재한다는 의미로 받아들일 수 있다. 반대로 $\frac{\partial C}{\partial z^l_j}$ 가 작으면 이미 $z^l_j$ 가 optimal 에 근접한 상태라고 생각할 수 있고, 그래서 오차가 적은 상태라고 생각할 수 있다.
      - 이런 관점에서 $\delta^l_j = \frac{\partial C}{\partial z^l_j}$ 를 특정 노드의 오차라고 생각할 수 있다
    - 왜 $\frac{\partial C}{\partial a^l_j}$ 를 안 쓰고 $\frac{\partial C}{\partial z^l_j}$ 를 오차($\delta$)로 사용하는가? - 단순히 대수적으로 풀어쓰기 어렵기 때문임 (to make the presentation of backpropagation a little more algebraically complicated)
- 역전파는 이 error $\delta^l_j$를 구하고, 그리고 $\delta^l_j$를 통해서 $\frac{\partial C}{\partial w_{jk}^l}$ 과 $\frac{\partial C}{\partial b_j^l}$ 를 구하는 방식으로 작동한다.

## Four fundemental equations
- chain rule from multivariable calculus 를 알면 쉽게 이해할 수 있다
- $$\delta^L_j = \frac{\partial C}{\partial a_j^L} \sigma'(z^L_j) \tag{BP1}$$
  - $$\delta^l_j = \frac{\partial C}{\partial z^l_j} \tag8$$
  - 위 8번 식을 변형하면
  - $$\delta^l_j = \sum_k{\frac{\partial C}{\partial{a_k^L}} \frac{\partial{a_k^L}}{\partial z^l_j}} \tag9$$
  - 이고, $j \neq k$ 이면 $\frac{\partial{a_k^L}}{\partial z^l_j}$ 가 0으로 사라진다. 따라서
  - $$\delta^L_j = \frac{\partial C}{\partial a_j^L} \frac{\partial a_j^L}{\partial z_j^L}= \frac{\partial C}{\partial a_j^L} \sigma'(z^L_j) \tag{BP1}$$
  - 이다.
- (BP2) 의 컨셉은 $\delta^l$ 을 $\delta^{l+1}$ 에 대한 식으로 전개하겠다는 것이다
  - 오차를 이전 layer 로 전파시킨다는 컨셉으로 이해하면 된다
  - $$\begin{align}\delta^l &= \frac{\partial C}{\partial z^l_j} \tag{8} \\ &= \sum_k {\frac{\partial C}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial z^l_j}} \notag \\ &= \sum_k {\frac{\partial z^{l+1}_k}{\partial z_k^l}} \delta^{l+1}_k \tag{10} \end{align}$$
  - $$ z_k^{l+1} = b_k^{l+1} + \sum_j{w^{l+1}_{kj} a^l_j} = b_k^{l+1} + \sum_j{w^{l+1}_{kj} \sigma(z_j^l)} \tag{11}$$
  - $${\frac{\partial z^{l+1}_k}{\partial z_k^l}} = w^{l+1}_{kj} \sigma'(z_j^l) \tag{12}$$
  - $$\therefore \delta^l_j = \sum_k {w_{kj}^{l+1} \delta_k^{l+1} \sigma'(z^l_j)} \tag{13}$$
- (BP3), (BP4) 는 식 (7) 을 통해 쉽게 구할 수 있다.

## SGD
- 