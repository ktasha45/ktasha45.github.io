---
layout: post
title:  boostcamp ai tech 4기 3주차 회고
categories: boostcamp
---

# boostcourse ai tech 4기 3주차 회고
## learned
- 
## thought
- 
## not boostcamp
- batchnorm 이후 relu를 적용할 때, batchnorm은 relu 그래프에서 0 앞쪽, 뒤쪽에 얼마나 데이터를 분포시킬 것인지를 학습한다?
  - 저번 주에 나왔던 질문. batchnorm을 relu(activation) 이후에 쓰기도 하고, 그게 성능이 더 좋은 경우도 있다고 함.
  - batchnorm 관련 질답을 했음. 류건 님과.
  - 

  batch마다 다른 분포를 같게 해주는
batch size가 커지면 batch마다 분포가 크게 안달라질텐데

 -> 배치마다 다른 분포를 같게 해주는 게 아니라 -> layer의 출력을 정규화하는 데에 효과가 있다?

어떤 배치에서, layer1 10~20 -> 출력을 모아주는 
-> 평균0 std1
감마 베타 -> 

다른 배치에서 layer1 여기서 어차피 비슷한 출력
 -> 배치 사이즈가 작으면 아닐 수도 있음.
 -> 

내부 공변량 변화를 정말 줄이는 것인가?
 -> 그게 아니고, loss landscape를 평탄하게 하는 것이다
 -> 

loss landscape 가 평탄해지는 것과 공변량 을 맞추는 것과는 연관이 없다



## miscellaneous
- 
## feeling
- 
