행벡터는 왜 해집합과 직교하는가

> - overparameterized $\left(N\text{<}\text{<}M\right)$ 선형회귀에서 gradient descent를 하면 항상 zero-loss에 수직으로 접근한다.
> - overparameterized 선형회귀에서 $\text{span}(x_1, ..., x_N)$ 은 zero-loss를 이루는 가중치 $\theta$들의 집합 ($M$차원 공간에서의 hyperplane)과 orthogonal하다는데 어떻게 그렇게 되는 건지 잘 이해가 안 됨.  
zero-loss를 이루는 가중치 $\theta$들의 집합 $\Theta_0 := \left\{\theta \in \mathbb{R}^M \ | \ \theta^\intercal\mathbf{x}_i - \mathsf{y}_i = 0, \forall{i} \in [N]\right\}$  
>- 변수가 2개, 데이터가 1개인 경우를 생각해보면 직교함을 쉽게 보일 수 있음. 어떻게 일반화할 수 있을까?

boostcamp1 포스팅에서 언급했던 내용이다.  

맥락은 다음과 같다. 
1. 데이터 개수$\left(N\right)$보다 파라미터 수$\left(M\right)$가 더 많은  $\left(N\text{<}\text{<}M\right)$ 선형 회귀 문제에서 SGD 의 궤도를 생각해보자.
2. 목적식은 다음과 같다. $$\min_{\theta \in \R^M} \sum_{i=1}^N \left(y_i-\theta^\intercal x_i\right)^2\\
x_i \in \R^M, \ y_i \in \R$$
3. gradient 는 다음과 같다. $$\nabla_\theta L \left(\theta\right) = \sum_{i=1}^N \gamma_i x_i\\
\gamma_i = -2\left(y - \theta^\intercal x_i\right)$$ 
4. SGD로 최적화된 $\theta^{\left(K\right)}$는 다음과 같다 $$\theta^{\left(K\right)} = \theta^{\left(0\right)} + \sum_{i=1}^N \zeta_i x_i$$
5. 

## 1. 왜 zero-loss에 수직으로 접근하는가
