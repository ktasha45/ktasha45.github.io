---
layout: post
title: 02 19
categories: zizz
---

02 18

어제는 langcon 에 갔다.  
상렬 선배도 뵀고, nlp 분들, 오수지 멘토님도 뵀다.  
너무 재미있었다. 진심으로  

또 뵀으면 좋겠다  
꼭

---

02 19

오늘은 CUAI 면접을 봤다.  
내 앞에 계신 분은 면접관이 5분이시던데, 나는 3분이셨다.  
운영진 중에 나를 아시는 분은 빠지신 것 같다.

- 들어가서 아이스브레이킹하고  
- 자기소개, 왜 지원했는지
  - 나는 몇학번 누구누구이고, 추천시스템과 강화학습에 관심있는데 이걸 같이 공부할 팀원들이 필요했고, CV, NLP 쪽으로도 다양한 분들이 모이시기에 그쪽으로도 같이 공부하며 대회도 나가고 하면 좋을 것 같아 지원했다
  - 고 했는데, 더 잘 이야기할 수 있었을 텐데.. 음. 근데 이건 준비하는 게 오히려 더 어색할 수도 있을 것 같다
- 책추천 대회에서 쓴 deepconn 이랑 FFM 에 대해 설명하시오  
  - deepconn 은 사용자가 남긴 리뷰 text 데이터를 추천에 활용하는 모델이고  
  - 우리는 대회에서 책의 summary 를 리뷰로 대체해서 사용했다고 했다.  
  - 그리고 FFM 은 FM 에서 발전한 모델으로, FM 은 일단 feature 간의 interaction 활용하는 모델이고  
  - FFM 은 그 과정에서 좀 더 field-aware 하게 interaction 을 모델링하는 것 - 나는 context 를 더 본다는 식으로 이야기했음..
  - 이것도 PCA 때와 마찬가지로 어디부터 해야할지 감이 안 잡히더라. 정리할 필요를 느꼈음
- 데이터 크롤링 하면서 에러는 어떤게 있었는지, 가장 어려웠던 에러는 뭔지  
  - 일단 데이터 어떻게 모았는지 2 stage 로 했다고 말씀드리고
  - api 호출할 때 데이터가 자꾸 없다고 하기도 하고, 돌려놓고 자면 새로운 에러가 나오고 그런 게 힘들었다고 말씀드렸음
- 생성모델에 대해 설명해라
  - gan 은 생성자 판별자가 adverserial 하게 학습하고, 생성자가 생성하면 판별자가 맞는지, 틀리는지를 판단한다. 그렇게 생성을 학습한다
  - diffusion 은 이미지를 노이즈로 만들고, 그 노이즈에서 이미지를 다시 복원하는 것을 NN 으로 근사해서 그렇게 생성을 학습한다.
  - rough 하게만 이해하고 있다고 말했음
- 모델을 통해 실제 가치 창출?
  - 부캠을 하면서 리서치 단에서 0.1%, 0.2% 를 올리는 것이 서비스적으론 크게 의미가 없을 수도 있다는 것을 느꼈음.
  - 서비스 측면에서 latency 를 줄인다든가 하는 쪽으로 실제 사용자에게 만족을 줄 수 있는 서비스를 만들어보고 싶다고
  - 하려고 했는데.. 조리있게 이야기하지 못한 듯. 그래도 대충 의미는 전달된 듯
- MML 스터디 했다고 했는데 가장 인상적이었던 것은?
  - 선형대수 부분과 차원축소에서의 PCA, EA
- PCA 에 대해 설명하시오 - 선형대수학적 관점에서
  - PCA 는 차원축소 기법이고, 새로운 축을 찾아 거기로 데이터를 projection 해서 차원을 축소시키는 방법이다
  - 이때 새로운 축은, 선형대수에서 소위 말하는 eigen 값들과 연관이 있고.. - 이정도까지만 말했는데 더 깊게 말해야 했을까 아쉽다
  - projection matrix 나 eigen 등의 선형대수학적 지식이 요구된다
  - 어디서부터 말씀드려야 할지 애매했고 공부한 것들을 정리할 필요를 느꼈다. 빨리 mml 포스팅 끝내야겠다.... 그러면서 좀 정리해야겠다
- 타이타닉 과제에서 파생변수나 이상치 전처리 안했는데 성능 잘나옴 왜?  
  - lgbm 자체 성능이 tabular 에서 압도적으로 좋기도 하고, 음 왜 좋은지는 잘 모르겠다. 부스팅 알고리즘에 대해 잘 알지 못해서 잘 모르겠다.
- 하이퍼파라미터 튜닝 열심히 했던데 어떤 걸 중심으로 했는가
  - lgbm 을 썼는데, 따로 뭔가를 중심으로 하진 않았다. 단지 알고있는 lgbm 하이퍼파라미터들을 넣었고, optuna 가 알아서 처리해줬다. 그뿐이다
  - num_estimator 나 num_leaves? 그냥 이런 아는 거 넣고 돌렸다.
- 데이터 크롤링, CICD 는 어떻게 했는지
  - 데이터 크롤링은 앞에서 말했듯 last.fm 에서 크롤링했고, CICD 는 crontab 이나 airflow 를 통해 그냥 밤 12시마다 데이터 새로 불러와서 DB에 적재하는 것이라 어렵지 않았다
  - 고 했는데.. 이건 좀 실언인 것 같다. 어려웠다. 특히 DB 스키마에 맞추는 게.. 실제로 완전히 끝내지도 못했고
  - bentoml 으로 배포했다는 이야기도 했다!
  - 다음부턴 좀 더 신중하게 대답하자. 진실되게
- 마지막으로 하고 싶은 말은?
  - 주말에 시간 내줘서 감사하고.... 다른 말은 하지 않았음

아마 빼먹은 질문도 있을 거고, 순서도 뒤죽박죽일 것이다. 아 녹음을 해놨어야 했는데..  
다음엔 시작하기 전에 물어보고 나도 녹음해야겠다. 운영진 측에선 녹음하던데 달라고 하면 줄라나-  
뭔가 아쉬움이 좀 남는다  
다음에 더 잘 하면 되지!

오늘은 FM 논문 읽고 아마 어제 langcon 회고하고.. AI 가속기 교육도 회고하고  
아마 그러면 하루가 끝날 것 같다. 시간 남으면 quantization 할 것 같다

---

first draft: 2023.02.119 16:09